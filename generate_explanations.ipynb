{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import vgg"
   ],
   "metadata": {
    "id": "e5B1CHbAnLiy"
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def spike_function(input):\n",
    "    out = torch.zeros_like(input).cuda()\n",
    "    out[input > 0] = 1.0\n",
    "    return out\n",
    "\n",
    "\n",
    "class svgg19(vgg.VGG):\n",
    "    def __init__(self, num_steps=25, leak_mem=0.95, img_size=224, num_cls=50):\n",
    "        super(svgg19, self).__init__(vgg.make_layers([64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"], False))\n",
    "        self.num_steps = num_steps\n",
    "        self.num_cls = num_cls\n",
    "        self.batch_num = self.num_steps\n",
    "        self.leak_mem = leak_mem\n",
    "        self.img_size = img_size\n",
    "\n",
    "\n",
    "        bias_flag = False\n",
    "\n",
    "        for m in self.modules():\n",
    "            if (isinstance(m, nn.Conv2d)):\n",
    "                m.threshold = 1.0\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=2)\n",
    "            elif (isinstance(m, nn.Linear)):\n",
    "                m.threshold = 1.0\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "            mem_conv1 = torch.zeros(batch_size, 64, self.img_size, self.img_size).cuda()\n",
    "            mem_conv2 = torch.zeros(batch_size, 64, self.img_size, self.img_size).cuda()\n",
    "            mem_conv3 = torch.zeros(batch_size, 128, self.img_size//2, self.img_size//2).cuda()\n",
    "            mem_conv4 = torch.zeros(batch_size, 128, self.img_size//2, self.img_size//2).cuda()\n",
    "            mem_conv5 = torch.zeros(batch_size, 256, self.img_size//4, self.img_size//4).cuda()\n",
    "            mem_conv6 = torch.zeros(batch_size, 256, self.img_size//4, self.img_size//4).cuda()\n",
    "            mem_conv7 = torch.zeros(batch_size, 256, self.img_size//4, self.img_size//4).cuda()\n",
    "            mem_conv8 = torch.zeros(batch_size, 256, self.img_size//4, self.img_size//4).cuda()\n",
    "            mem_conv9 = torch.zeros(batch_size, 512, self.img_size//8, self.img_size//8).cuda()\n",
    "            mem_conv10 = torch.zeros(batch_size, 512, self.img_size//8, self.img_size//8).cuda()\n",
    "            mem_conv11 = torch.zeros(batch_size, 512, self.img_size//8, self.img_size//8).cuda()\n",
    "            mem_conv12 = torch.zeros(batch_size, 512, self.img_size//8, self.img_size//8).cuda()\n",
    "            mem_conv13 = torch.zeros(batch_size, 512, self.img_size//16, self.img_size//16).cuda()\n",
    "            mem_conv14 = torch.zeros(batch_size, 512, self.img_size//16, self.img_size//16).cuda()\n",
    "            mem_conv15 = torch.zeros(batch_size, 512, self.img_size//16, self.img_size//16).cuda()\n",
    "            mem_conv16 = torch.zeros(batch_size, 512, self.img_size//16, self.img_size//16).cuda()\n",
    "            mem_conv_list = [mem_conv1, mem_conv2, mem_conv3, mem_conv4, mem_conv5, mem_conv6, mem_conv7, mem_conv8, mem_conv9, mem_conv10, mem_conv11, mem_conv12, mem_conv13, mem_conv14, mem_conv15, mem_conv16]\n",
    "\n",
    "            mem_fc1 = torch.zeros(batch_size, 4096).cuda()\n",
    "            mem_fc2 = torch.zeros(batch_size, 4096).cuda()\n",
    "            mem_fc3 = torch.zeros(batch_size, self.num_cls).cuda()\n",
    "\n",
    "            mem_fc_list = [mem_fc1, mem_fc2, mem_fc3]\n",
    "\n",
    "            # To collect all activation maps and return\n",
    "            all_activation_maps = [list() for conv in mem_conv_list]\n",
    "\n",
    "            for t in range(self.num_steps):\n",
    "                out_prev = x\n",
    "\n",
    "                # Keep track of which actual CONV layer were currently pushing input through\n",
    "                conv_layer_counter = 0\n",
    "                for module_idx, module in enumerate(self.features):\n",
    "                    if isinstance(module, nn.Conv2d):\n",
    "                      out_prev = module(out_prev)\n",
    "                    # Replace ReLU by the spiking activation function\n",
    "                    elif isinstance(module, nn.ReLU):\n",
    "                      mem_conv_list[conv_layer_counter] = self.leak_mem * mem_conv_list[conv_layer_counter] + out_prev\n",
    "                      mem_thr = (mem_conv_list[conv_layer_counter] / self.features[module_idx-1].threshold) - 1.0\n",
    "                      out = spike_function(mem_thr)\n",
    "                      rst = torch.zeros_like(mem_conv_list[conv_layer_counter]).cuda()\n",
    "                      rst[mem_thr > 0] = self.features[module_idx-1].threshold\n",
    "                      mem_conv_list[conv_layer_counter] = mem_conv_list[conv_layer_counter] - rst\n",
    "                      out_prev = out.clone()\n",
    "\n",
    "                      # To collect\n",
    "                      all_activation_maps[conv_layer_counter].append(out_prev.detach())\n",
    "\n",
    "                      conv_layer_counter += 1\n",
    "                    elif isinstance(module, nn.MaxPool2d):\n",
    "                      out = module(out_prev)\n",
    "                      out_prev = out.clone()\n",
    "\n",
    "                out = self.avgpool(out_prev)\n",
    "                out_prev = out.clone()\n",
    "\n",
    "                out_prev = out_prev.reshape(batch_size, -1)\n",
    "\n",
    "                fc_counter = 0\n",
    "                for module_idx, module in enumerate(self.classifier):\n",
    "                      if isinstance(module, nn.Linear):\n",
    "                        out_prev = module(out_prev)\n",
    "                      elif isinstance(module, nn.ReLU):\n",
    "                          mem_fc_list[fc_counter] = self.leak_mem * mem_fc_list[fc_counter] + out_prev\n",
    "                          mem_thr = (mem_fc_list[fc_counter] / self.classifier[module_idx-1].threshold) - 1.0\n",
    "                          out = spike_function(mem_thr)\n",
    "                          rst = torch.zeros_like(mem_fc_list[fc_counter]).cuda()\n",
    "                          rst[mem_thr > 0] = self.classifier[module_idx-1].threshold\n",
    "                          mem_fc_list[fc_counter] = mem_fc_list[fc_counter] - rst\n",
    "                          out_prev = out.clone()\n",
    "                          fc_counter += 1\n",
    "                      elif isinstance(module, nn.Dropout):\n",
    "                        out_prev = module(out_prev)\n",
    "\n",
    "                mem_fc_list[-1] = mem_fc_list[-1] + out_prev\n",
    "\n",
    "            average_activation_spikemaps = [torch.mean(torch.stack(s, dim=0).permute(1, 0, 2, 3, 4), dim=1) for s in all_activation_maps]\n",
    "\n",
    "            out_voltage = mem_fc_list[-1] / self.num_steps\n",
    "\n",
    "            return out_voltage, average_activation_spikemaps"
   ],
   "metadata": {
    "id": "sCp0bZvmnPdX"
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define model\n",
    "model = svgg19(num_steps=25, leak_mem=0.95, img_size=224, num_cls=50)\n",
    "model.classifier._modules['6'] = nn.Linear(4096, 50)\n",
    "model.load_state_dict(torch.load(\"./drive/MyDrive/Thesis/SNN_grad/svgg19_AwA2.pth\"))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sn8CrGFpnnTC",
    "outputId": "5a05fcd2-1c06-4c1d-867e-f940a9f1173e"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "svgg19(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load w for relevant filters in model\n",
    "with open('./drive/MyDrive/Thesis/generate_explanation/lambda20_w.npy', 'rb') as file_add:\n",
    "        w = pickle.load(file_add)\n",
    "\n",
    "# Decode w\n",
    "w_decoded = []\n",
    "\n",
    "list_of_conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
    "for class_index in range(50):\n",
    "    w_row = w[class_index]\n",
    "\n",
    "    non_zero_w_indices = torch.nonzero(torch.from_numpy(w[class_index]))\n",
    "    sub_w_rows = []\n",
    "    current_w_start_idx = 0\n",
    "    for conv_idx in range(len(list_of_conv_layers)):\n",
    "        to_append = []\n",
    "        current_w_end_idx = current_w_start_idx + list_of_conv_layers[conv_idx].out_channels\n",
    "        for non_zero_w_idx in non_zero_w_indices:\n",
    "            if current_w_start_idx <= non_zero_w_idx < current_w_end_idx:\n",
    "                to_append.append(non_zero_w_idx - current_w_start_idx)\n",
    "        sub_w_rows.append(to_append)\n",
    "        current_w_start_idx = current_w_end_idx\n",
    "    w_decoded.append(sub_w_rows)"
   ],
   "metadata": {
    "id": "kazlnijAnyJA"
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load image\n",
    "from PIL import Image\n",
    "\n",
    "# Replace this with the image from which you want to generate explanatiosn\n",
    "input_image_path = \"./unnamed.jpg\"\n",
    "image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Transform image and prepare for pushing through model\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = transform(image)  # shape: [C, H, W]\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Put image on correct device\n",
    "input_batch = input_batch.to(device)"
   ],
   "metadata": {
    "id": "3drDNsOgpJkm"
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Push image through model and collect predictions + the average activation maps\n",
    "with torch.no_grad():\n",
    "    predictions, average_activation_maps = model(input_batch)"
   ],
   "metadata": {
    "id": "AU1GgZZ0p__2"
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Select relevant filters from the predicted class\n",
    "predicted_class = torch.argmax(predictions, dim=1)\n",
    "relevant_filters = w_decoded[predicted_class]\n",
    "print(relevant_filters)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fUUHECktjPa",
    "outputId": "23a9c146-2162-477a-da2c-1301cfec3655"
   },
   "execution_count": 73,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[tensor([42])], [tensor([1])], [], [], [], [], [], [], [], [], [], [], [tensor([469])], [tensor([327])], [tensor([241])], [tensor([21]), tensor([123]), tensor([153]), tensor([434]), tensor([481]), tensor([500])]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "### Helper functions for generating the heatmap superimposed on the input image ###\n",
    "\n",
    "def write_image(write_add, A):\n",
    "    cv2.imwrite(write_add, np.uint8(A * 255))\n",
    "\n",
    "#superimposing heatmap (i.e. colored featuremap) on the input image\n",
    "def superimposing(image, heatmap):\n",
    "    vis = cv2.addWeighted(heatmap, 0.6, image, 0.4, 0)\n",
    "    return vis / np.max(vis)\n",
    "\n",
    "\n",
    "# apply color map on resized featuremap\n",
    "def apply_colormap(A):\n",
    "    A = cv2.applyColorMap(np.uint8(255 * A), cv2.COLORMAP_JET)\n",
    "    return np.float32(A) / 255\n",
    "\n",
    "#normalizing a map\n",
    "def normalize_numpy(A):\n",
    "    if np.max(A) == 0.0:\n",
    "        return A\n",
    "    return (A - np.min(A)) / (np.max(A) - np.min(A))\n",
    "\n",
    "#reading input image\n",
    "def read_image(image_address, shape):\n",
    "    image_name = image_address.split('/')[-1]\n",
    "    image = cv2.imread(image_address)\n",
    "\n",
    "    image = cv2.resize(image, dsize=(shape, shape))\n",
    "    image = np.float32(image)\n",
    "    image = normalize_numpy(image)\n",
    "    image = image[:, :, ::-1]\n",
    "    return image, image_name"
   ],
   "metadata": {
    "id": "l2-jyL4V0lkZ"
   },
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate explanatory heatmaps\n",
    "for layer_idx, relevant_filters_per_layer in enumerate(relevant_filters):\n",
    "  if len(relevant_filters_per_layer) != 0:\n",
    "    summed_activation_map = torch.zeros_like(average_activation_maps[layer_idx][0][0])\n",
    "    for relevant_filter in relevant_filters_per_layer:\n",
    "      summed_activation_map += average_activation_maps[layer_idx][0][relevant_filter.item()]\n",
    "\n",
    "      upsampled_map = cv2.resize(summed_activation_map.cpu().detach().numpy(), dsize=(224, 224))\n",
    "      upsampled_map = normalize_numpy(upsampled_map)\n",
    "      heatmap = apply_colormap(upsampled_map)\n",
    "      image, image_name = read_image(input_image_path, 224)\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      vis = superimposing(image, heatmap)\n",
    "      write_image('./drive/MyDrive/Thesis/generate_explanation/' + str(layer_idx) + '.jpg', vis)"
   ],
   "metadata": {
    "id": "V1JoPYP6v8Ul"
   },
   "execution_count": 85,
   "outputs": []
  }
 ]
}
